INFO:     Started server process [64001]
INFO:     Waiting for application startup.
Secure MCP Filesystem Server running on stdio
Updated allowed directories from MCP roots: 2 valid directories
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
Starting MCP Manager...
Connected to Obsidian MCP server.
Connected to Filesystem MCP server.
Scanning for tools in /Users/joshua/my-docs/code/elocuency-v3/workspace/tools...
Loading tool module: echo from /Users/joshua/my-docs/code/elocuency-v3/workspace/tools/echo.py
  Found tool: echo_tool
Binding 30 tools (27 MCP, 1 local) to AI adapter.
INFO:     127.0.0.1:50077 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50149 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50262 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50338 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50425 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:50425 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:50425 - "GET /agent/playground/assets/index-53ad47d4.js HTTP/1.1" 200 OK
INFO:     127.0.0.1:50427 - "GET /agent/playground/assets/index-434ff580.css HTTP/1.1" 200 OK
INFO:     127.0.0.1:50427 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:50427 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:50427 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50554 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50666 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50789 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:50789 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:50791 - "GET /agent/playground/assets/index-53ad47d4.js HTTP/1.1" 200 OK
INFO:     127.0.0.1:50789 - "GET /agent/playground/assets/index-434ff580.css HTTP/1.1" 200 OK
INFO:     127.0.0.1:50789 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:50791 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:50879 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Entering astream_log. Input type: <class 'dict'>
DEBUG: Calling graph.astream_log with config keys: dict_keys(['tags', 'metadata', 'configurable', 'run_name', 'run_id'])
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 1
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Got chunk from graph. Ops count: 2
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:50997 - "POST /agent/stream_log HTTP/1.1" 200 OK
