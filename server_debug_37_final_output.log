INFO:     Started server process [62300]
INFO:     Waiting for application startup.
Secure MCP Filesystem Server running on stdio
Updated allowed directories from MCP roots: 2 valid directories
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
Starting MCP Manager...
Connected to Obsidian MCP server.
Connected to Filesystem MCP server.
Scanning for tools in /Users/joshua/my-docs/code/elocuency-v3/workspace/tools...
Loading tool module: echo from /Users/joshua/my-docs/code/elocuency-v3/workspace/tools/echo.py
  Found tool: echo_tool
Binding 30 tools (27 MCP, 1 local) to AI adapter.
INFO:     127.0.0.1:62479 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:62540 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:62604 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:62702 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:62702 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:62704 - "GET /agent/playground/assets/index-53ad47d4.js HTTP/1.1" 200 OK
INFO:     127.0.0.1:62706 - "GET /agent/playground/assets/index-434ff580.css HTTP/1.1" 200 OK
INFO:     127.0.0.1:62706 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:62706 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:62743 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:62842 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:62955 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:63437 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:63533 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:63626 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:63694 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:63694 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:63696 - "GET /agent/playground/assets/index-53ad47d4.js HTTP/1.1" 200 OK
INFO:     127.0.0.1:63694 - "GET /agent/playground/assets/index-434ff580.css HTTP/1.1" 200 OK
INFO:     127.0.0.1:63694 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:63694 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:63743 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:63848 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:63943 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:64396 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:64396 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:64396 - "GET /agent/playground/assets/index-53ad47d4.js HTTP/1.1" 200 OK
INFO:     127.0.0.1:64398 - "GET /agent/playground/assets/index-434ff580.css HTTP/1.1" 200 OK
INFO:     127.0.0.1:64398 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:64396 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:64443 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:64514 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:64516 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:64518 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:64630 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:64786 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:64786 - "GET /agent/playground/assets/index-53ad47d4.js HTTP/1.1" 200 OK
INFO:     127.0.0.1:64788 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:64791 - "GET /agent/playground/assets/index-434ff580.css HTTP/1.1" 200 OK
INFO:     127.0.0.1:64791 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:64788 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:64817 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:65215 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:65296 - "GET /agent/playground/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:65296 - "GET /.well-known/appspecific/com.chrome.devtools.json HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:65298 - "GET /agent/playground/assets/index-53ad47d4.js HTTP/1.1" 200 OK
INFO:     127.0.0.1:65301 - "GET /agent/playground/assets/index-434ff580.css HTTP/1.1" 200 OK
INFO:     127.0.0.1:65301 - "GET /agent/c/N4XyA/input_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:65301 - "GET /agent/c/N4XyA/output_schema HTTP/1.1" 200 OK
INFO:     127.0.0.1:65336 - "POST /agent/stream_log HTTP/1.1" 200 OK
NotImplementedError in LogStreamCallbackHandler.on_llm_error callback: NotImplementedError("Trying to load an object that doesn't implement serialization: {'lc': 1, 'type': 'not_implemented', 'id': [], 'repr': '<coroutine object ClientResponse.json at 0x1279c0c80>'}")
Traceback (most recent call last):
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/src/infrastructure/adapters/ai/langgraph_agent_adapter.py", line 142, in astream_log
    }
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1262, in astream_log
    async for item in _astream_log_implementation(  # type: ignore[call-overload]
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 768, in _astream_log_implementation
    await task
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 720, in consume_astream
    async for chunk in runnable.astream(value, config, **kwargs):
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/main.py", line 2974, in astream
    async for _ in runner.atick(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/_runner.py", line 304, in atick
    await arun_with_retry(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/_retry.py", line 133, in arun_with_retry
    async for _ in task.proc.astream(task.input, config):
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 871, in astream
    output = await _consume_aiter(aiterator)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 904, in _consume_aiter
    async for chunk in it:
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 337, in tap_output_aiter
    async for chunk in output:
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1589, in atransform
    async for ichunk in input:
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1589, in atransform
    async for ichunk in input:
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1170, in astream
    yield await self.ainvoke(input, config, **kwargs)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 466, in ainvoke
    ret = await coro
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/prebuilt/chat_agent_executor.py", line 701, in acall_model
    response = cast(AIMessage, await static_model.ainvoke(model_input, config))  # type: ignore[union-attr]
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3197, in ainvoke
    input_ = await coro_with_context(part(), context, create_task=True)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5708, in ainvoke
    return await self.bound.ainvoke(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 425, in ainvoke
    llm_result = await self.agenerate_prompt(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1132, in agenerate_prompt
    return await self.agenerate(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1090, in agenerate
    raise exceptions[0]
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1316, in _agenerate_with_cache
    async for chunk in self._astream(messages, stop=stop, **kwargs):
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 3203, in _astream
    stream = await self.client.aio.models.generate_content_stream(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/models.py", line 7483, in generate_content_stream
    response = await self._generate_content_stream(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/models.py", line 6188, in _generate_content_stream
    response_stream = await self._api_client.async_request_streamed(
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/_api_client.py", line 1459, in async_request_streamed
    response = await self._async_request(http_request=http_request, stream=True)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/_api_client.py", line 1375, in _async_request
    return await self._async_retry(  # type: ignore[no-any-return]
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 111, in __call__
    do = await self.iter(retry_state=retry_state)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 114, in __call__
    result = await fn(*args, **kwargs)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/_api_client.py", line 1291, in _async_request_once
    await errors.APIError.raise_for_async_response(response)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/errors.py", line 216, in raise_for_async_response
    await cls.raise_error_async(status_code, response_json, response)
  File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/errors.py", line 240, in raise_error_async
    raise ServerError(status_code, response_json, response)
google.genai.errors.ServerError: 503 Service Unavailable. {'message': '{\n  "error": {\n    "code": 503,\n    "message": "The service is currently unavailable.",\n    "status": "UNAVAILABLE"\n  }\n}\n', 'status': 'Service Unavailable'}
ERROR:    Exception in ASGI application
  + Exception Group Traceback (most recent call last):
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py", line 416, in run_asgi
  |     result = await app(  # type: ignore[func-returns-value]
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
  |     return await self.app(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/fastapi/applications.py", line 1054, in __call__
  |     await super().__call__(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/applications.py", line 123, in __call__
  |     await self.middleware_stack(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 186, in __call__
  |     raise exc
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 164, in __call__
  |     await self.app(scope, receive, _send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/cors.py", line 91, in __call__
  |     await self.simple_response(scope, receive, send, request_headers=headers)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/cors.py", line 146, in simple_response
  |     await self.app(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py", line 62, in __call__
  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 64, in wrapped_app
  |     raise exc
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
  |     await app(scope, receive, sender)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 758, in __call__
  |     await self.middleware_stack(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 778, in app
  |     await route.handle(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 299, in handle
  |     await self.app(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 79, in app
  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 64, in wrapped_app
  |     raise exc
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
  |     await app(scope, receive, sender)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 77, in app
  |     await response(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/sse_starlette/sse.py", line 255, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 783, in __aexit__
  |     raise BaseExceptionGroup(
  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/sse_starlette/sse.py", line 258, in wrap
    |     await func()
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/sse_starlette/sse.py", line 245, in stream_response
    |     async for data in self.body_iterator:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langserve/api_handler.py", line 1277, in _stream_log
    |     async for chunk in self._runnable.astream_log(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/src/infrastructure/adapters/ai/langgraph_agent_adapter.py", line 222, in astream_log
    |     if val["messages"]:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/src/infrastructure/adapters/ai/langgraph_agent_adapter.py", line 142, in astream_log
    |     }
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1262, in astream_log
    |     async for item in _astream_log_implementation(  # type: ignore[call-overload]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 768, in _astream_log_implementation
    |     await task
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 720, in consume_astream
    |     async for chunk in runnable.astream(value, config, **kwargs):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/main.py", line 2974, in astream
    |     async for _ in runner.atick(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/_runner.py", line 304, in atick
    |     await arun_with_retry(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/_retry.py", line 133, in arun_with_retry
    |     async for _ in task.proc.astream(task.input, config):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 871, in astream
    |     output = await _consume_aiter(aiterator)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 904, in _consume_aiter
    |     async for chunk in it:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 337, in tap_output_aiter
    |     async for chunk in output:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1589, in atransform
    |     async for ichunk in input:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1589, in atransform
    |     async for ichunk in input:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1170, in astream
    |     yield await self.ainvoke(input, config, **kwargs)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 466, in ainvoke
    |     ret = await coro
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/prebuilt/chat_agent_executor.py", line 701, in acall_model
    |     response = cast(AIMessage, await static_model.ainvoke(model_input, config))  # type: ignore[union-attr]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3197, in ainvoke
    |     input_ = await coro_with_context(part(), context, create_task=True)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5708, in ainvoke
    |     return await self.bound.ainvoke(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 425, in ainvoke
    |     llm_result = await self.agenerate_prompt(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1132, in agenerate_prompt
    |     return await self.agenerate(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1090, in agenerate
    |     raise exceptions[0]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1316, in _agenerate_with_cache
    |     async for chunk in self._astream(messages, stop=stop, **kwargs):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 3203, in _astream
    |     stream = await self.client.aio.models.generate_content_stream(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/models.py", line 7483, in generate_content_stream
    |     response = await self._generate_content_stream(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/models.py", line 6188, in _generate_content_stream
    |     response_stream = await self._api_client.async_request_streamed(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/_api_client.py", line 1459, in async_request_streamed
    |     response = await self._async_request(http_request=http_request, stream=True)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/_api_client.py", line 1375, in _async_request
    |     return await self._async_retry(  # type: ignore[no-any-return]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 111, in __call__
    |     do = await self.iter(retry_state=retry_state)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    |     result = await action(retry_state)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    |     return call(*args, **kwargs)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    |     raise retry_exc.reraise()
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    |     raise self.last_attempt.result()
    |   File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    |     return self.__get_result()
    |   File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    |     raise self._exception
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 114, in __call__
    |     result = await fn(*args, **kwargs)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/_api_client.py", line 1291, in _async_request_once
    |     await errors.APIError.raise_for_async_response(response)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/errors.py", line 216, in raise_for_async_response
    |     await cls.raise_error_async(status_code, response_json, response)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/google/genai/errors.py", line 240, in raise_error_async
    |     raise ServerError(status_code, response_json, response)
    | google.genai.errors.ServerError: 503 Service Unavailable. {'message': '{\n  "error": {\n    "code": 503,\n    "message": "The service is currently unavailable.",\n    "status": "UNAVAILABLE"\n  }\n}\n', 'status': 'Service Unavailable'}
    +------------------------------------
ERROR in astream_log: 503 Service Unavailable. {'message': '{\n  "error": {\n    "code": 503,\n    "message": "The service is currently unavailable.",\n    "status": "UNAVAILABLE"\n  }\n}\n', 'status': 'Service Unavailable'}
INFO:     127.0.0.1:65452 - "POST /agent/stream_log HTTP/1.1" 200 OK
DEBUG: Inspecting final_output value type: <class 'dict'>
DEBUG: final_output keys: dict_keys(['agent'])
DEBUG: Found last message for final_output: <class 'langchain_core.messages.ai.AIMessage'>
INFO:     127.0.0.1:65533 - "POST /agent/stream_log HTTP/1.1" 200 OK
/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/pydantic/main.py:271: RuntimeWarning: coroutine 'ClientResponse.json' was never awaited
  return getattr(cls, '__pydantic_fields__', {})
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
