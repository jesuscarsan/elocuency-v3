INFO:     Started server process [55837]
INFO:     Waiting for application startup.
Secure MCP Filesystem Server running on stdio
Updated allowed directories from MCP roots: 2 valid directories
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
Starting MCP Manager...
Connected to Obsidian MCP server.
Connected to Filesystem MCP server.
Scanning for tools in /Users/joshua/my-docs/code/elocuency-v3/workspace/tools...
Loading tool module: echo from /Users/joshua/my-docs/code/elocuency-v3/workspace/tools/echo.py
  Found tool: echo_tool
Binding 30 tools (27 MCP, 1 local) to AI adapter.
INFO:     127.0.0.1:51756 - "POST /agent/stream_log HTTP/1.1" 200 OK
ERROR:    Exception in ASGI application
  + Exception Group Traceback (most recent call last):
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py", line 416, in run_asgi
  |     result = await app(  # type: ignore[func-returns-value]
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
  |     return await self.app(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/fastapi/applications.py", line 1054, in __call__
  |     await super().__call__(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/applications.py", line 123, in __call__
  |     await self.middleware_stack(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 186, in __call__
  |     raise exc
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 164, in __call__
  |     await self.app(scope, receive, _send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/cors.py", line 83, in __call__
  |     await self.app(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py", line 62, in __call__
  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 64, in wrapped_app
  |     raise exc
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
  |     await app(scope, receive, sender)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 758, in __call__
  |     await self.middleware_stack(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 778, in app
  |     await route.handle(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 299, in handle
  |     await self.app(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 79, in app
  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 64, in wrapped_app
  |     raise exc
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
  |     await app(scope, receive, sender)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/starlette/routing.py", line 77, in app
  |     await response(scope, receive, send)
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/sse_starlette/sse.py", line 255, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 783, in __aexit__
  |     raise BaseExceptionGroup(
  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/callbacks/manager.py", line 383, in _ahandle_event_for_handler
    |     await asyncio.get_event_loop().run_in_executor(
    |   File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    |     result = self.fn(*self.args, **self.kwargs)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/base.py", line 79, in on_chat_model_start
    |     chat_model_run = self._create_chat_model_run(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/core.py", line 175, in _create_chat_model_run
    |     raise NotImplementedError(msg)
    | NotImplementedError: Chat model tracing is not supported in for original format.
    | 
    | During handling of the above exception, another exception occurred:
    | 
    | Traceback (most recent call last):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/sse_starlette/sse.py", line 258, in wrap
    |     await func()
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/sse_starlette/sse.py", line 245, in stream_response
    |     async for data in self.body_iterator:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langserve/api_handler.py", line 1277, in _stream_log
    |     async for chunk in self._runnable.astream_log(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1262, in astream_log
    |     async for item in _astream_log_implementation(  # type: ignore[call-overload]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 768, in _astream_log_implementation
    |     await task
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 720, in consume_astream
    |     async for chunk in runnable.astream(value, config, **kwargs):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/main.py", line 2974, in astream
    |     async for _ in runner.atick(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/_runner.py", line 304, in atick
    |     await arun_with_retry(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/pregel/_retry.py", line 133, in arun_with_retry
    |     async for _ in task.proc.astream(task.input, config):
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 871, in astream
    |     output = await _consume_aiter(aiterator)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 904, in _consume_aiter
    |     async for chunk in it:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py", line 337, in tap_output_aiter
    |     async for chunk in output:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1589, in atransform
    |     async for ichunk in input:
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1170, in astream
    |     yield await self.ainvoke(input, config, **kwargs)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py", line 466, in ainvoke
    |     ret = await coro
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langgraph/prebuilt/chat_agent_executor.py", line 701, in acall_model
    |     response = cast(AIMessage, await static_model.ainvoke(model_input, config))  # type: ignore[union-attr]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3197, in ainvoke
    |     input_ = await coro_with_context(part(), context, create_task=True)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 425, in ainvoke
    |     llm_result = await self.agenerate_prompt(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1132, in agenerate_prompt
    |     return await self.agenerate(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1039, in agenerate
    |     run_managers = await callback_manager.on_chat_model_start(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/callbacks/manager.py", line 1940, in on_chat_model_start
    |     await asyncio.gather(*non_inline_tasks)
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/callbacks/manager.py", line 442, in ahandle_event
    |     await asyncio.gather(
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/callbacks/manager.py", line 392, in _ahandle_event_for_handler
    |     message_strings = [get_buffer_string(m) for m in args[1]]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/callbacks/manager.py", line 392, in <listcomp>
    |     message_strings = [get_buffer_string(m) for m in args[1]]
    |   File "/Users/joshua/my-docs/code/elocuency-v3/apps/elo-server/venv/lib/python3.10/site-packages/langchain_core/messages/utils.py", line 427, in get_buffer_string
    |     raise ValueError(msg)  # noqa: TRY004
    | ValueError: Got unsupported message type: content='Hello' additional_kwargs={} response_metadata={} type='human' id='69b39d6f-3411-4e46-868b-4a29049d91e6'
    +------------------------------------
